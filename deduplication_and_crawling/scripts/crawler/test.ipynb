{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b354d3",
   "metadata": {},
   "source": [
    "# Test the Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b51f7",
   "metadata": {},
   "source": [
    "Extracting\n",
    "- page_id\n",
    "- node_id\n",
    "- outgoing links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57922b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported parse_page function from parser.py\n",
      "\n",
      "--- üöÄ Running Test Step 1: v1.2 Parser Test ---\n",
      "Fetching http://localhost:3000/...\n",
      "Fetch successful. HTTP Status: 200\n",
      "\n",
      "Parsing HTML content with new parser...\n",
      "‚úÖ‚úÖ‚úÖ Parse Successful! ‚úÖ‚úÖ‚úÖ\n",
      "  Page ID:   page_ondib3z5\n",
      "  Node ID:   yrlelpggirpt\n",
      "  Num Links: 5\n",
      "  Links:     ['page_ql618swi', 'page_idxild28', 'page_2x0nyoul', 'page_q0mdnv7q', 'page_58bjp2st']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sys\n",
    "\n",
    "# --- Test Step 1: HTML Parser (v1.2) ---\n",
    "# Goal: Test the *new* parser on the portal page ('/')\n",
    "# We expect this to pass, as the portal page contains all the data.\n",
    "\n",
    "try:\n",
    "    # This will now import our new parser function\n",
    "    from parser import parse_page\n",
    "    print(\"Successfully imported parse_page function from parser.py\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå FAILED TO IMPORT 'parser'. Make sure 'parser.py' is in the same folder as this notebook.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "BASE_URL = \"http://localhost:3000\"\n",
    "\n",
    "def test_step_1_parser():\n",
    "    \"\"\"\n",
    "    Tests the new v1.2 parser on the portal page '/'.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üöÄ Running Test Step 1: v1.2 Parser Test ---\")\n",
    "    \n",
    "    target_url = f\"{BASE_URL}/\" \n",
    "\n",
    "    try:\n",
    "        print(f\"Fetching {target_url}...\")\n",
    "        response = requests.get(target_url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        print(\"Fetch successful. HTTP Status:\", response.status_code)\n",
    "        \n",
    "        # --- Test the Parser ---\n",
    "        print(\"\\nParsing HTML content with new parser...\")\n",
    "        parsed_data = parse_page(response.text)\n",
    "        \n",
    "        if parsed_data:\n",
    "            print(\"‚úÖ‚úÖ‚úÖ Parse Successful! ‚úÖ‚úÖ‚úÖ\")\n",
    "            print(f\"  Page ID:   {parsed_data.get('page_id')}\")\n",
    "            print(f\"  Node ID:   {parsed_data.get('node_id')}\")\n",
    "            print(f\"  Num Links: {len(parsed_data.get('links', []))}\")\n",
    "            print(f\"  Links:     {parsed_data.get('links')}\")\n",
    "        else:\n",
    "            print(\"‚ùå‚ùå‚ùå Parse Failed. Check 'parser.py' and its error messages.\")\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"\\n‚ùå TEST FAILED: Connection Error.\")\n",
    "        print(f\"Could not connect to {BASE_URL}.\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\n‚ùå TEST FAILED: An error occurred: {e}\")\n",
    "\n",
    "# --- Run the test ---\n",
    "test_step_1_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5587b0b",
   "metadata": {},
   "source": [
    "# Test The Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdd70a",
   "metadata": {},
   "source": [
    "This crawler must:\n",
    "- Start by visiting / (or /page_xxx)\n",
    "- Parse page_id, node_id, outgoing links\n",
    "- Discover new pages via BFS\n",
    "- Store:\n",
    "    - graph (adjacency list)\n",
    "    - latest node_id\n",
    "    - timestamps\n",
    "- Track visit count\n",
    "- Avoid revisiting pages too frequently\n",
    "\n",
    "And it must use aiohttp + asyncio for speed.\n",
    "\n",
    "Here, what is provided:\n",
    "- BFS Discovery\n",
    "    Fast traversal of the entire graph.\n",
    "- Node ID Tracking\n",
    "    Stored in self.node_ids.\n",
    "- Visit Times\n",
    "    Used for staleness estimation.\n",
    "- revisit_pages()\n",
    "    Allows refreshing node_ids with minimal revisits later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986eb485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Crawler class from crawler.py\n",
      "Starting async crawler test...\n",
      "--- üöÄ Starting crawl from / ---\n",
      "\n",
      "--- ‚úÖ Crawl Finished ---\n",
      "  Total pages found: 55\n",
      "  Total visits made: 56\n",
      "  Time taken:        1.40 seconds\n",
      "\n",
      "--- Node Data (Sample) ---\n",
      "  page_ondib3z5: yrlelpggirpt\n",
      "  page_ql618swi: a9y21zjt0crr\n",
      "  page_idxild28: z39371o0rkua\n",
      "  page_2x0nyoul: bdwtz31ug689\n",
      "  page_q0mdnv7q: dgmd3gmf82ee\n",
      "...\n",
      "\n",
      "--- Crawler State (for inspection) ---\n",
      "Graph size: 55\n",
      "Node data size: 55\n",
      "Async crawler test finished.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# This patch is needed for Jupyter notebooks to run asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Test Step 2: Asynchronous Crawler ---\n",
    "# Goal: Run the crawler and discover the entire graph.\n",
    "\n",
    "try:\n",
    "    from crawler import Crawler\n",
    "    print(\"Successfully imported Crawler class from crawler.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå FAILED TO IMPORT 'Crawler'. Make sure 'crawler.py' is in the same folder.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    # Stop execution if import fails\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "async def test_step_2_crawler():\n",
    "    \"\"\"\n",
    "    Initializes and runs the crawler.\n",
    "    \"\"\"\n",
    "    # We can start from '/' which is the portal page\n",
    "    crawler = Crawler(start_page=\"/\") \n",
    "    await crawler.crawl()\n",
    "    \n",
    "    print(\"\\n--- Crawler State (for inspection) ---\")\n",
    "    print(f\"Graph size: {len(crawler.graph)}\")\n",
    "    print(f\"Node data size: {len(crawler.node_data)}\")\n",
    "    \n",
    "    # You can uncomment this to see the full graph\n",
    "    # print(\"\\nFull Graph:\")\n",
    "    # print(crawler.graph)\n",
    "\n",
    "# --- Run the test ---\n",
    "# In a notebook, we must run the async function like this:\n",
    "print(\"Starting async crawler test...\")\n",
    "asyncio.run(test_step_2_crawler())\n",
    "print(\"Async crawler test finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5a3f8",
   "metadata": {},
   "source": [
    "# Test The PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b3c4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Crawler class from crawler.py\n",
      "Successfully imported calculate_pagerank from pagerank.py\n",
      "Starting PageRank test (includes full crawl)...\n",
      "\n",
      "--- üöÄ Running Crawler to get graph... ---\n",
      "--- üöÄ Starting crawl from / ---\n",
      "\n",
      "--- ‚úÖ Crawl Finished ---\n",
      "  Total pages found: 55\n",
      "  Total visits made: 56\n",
      "  Time taken:        1.33 seconds\n",
      "\n",
      "--- Node Data (Sample) ---\n",
      "  page_ondib3z5: yrlelpggirpt\n",
      "  page_ql618swi: a9y21zjt0crr\n",
      "  page_idxild28: z39371o0rkua\n",
      "  page_2x0nyoul: bdwtz31ug689\n",
      "  page_q0mdnv7q: dgmd3gmf82ee\n",
      "...\n",
      "\n",
      "--- ‚úÖ Crawler finished. Found 55 pages. ---\n",
      "--- üöÄ Calculating PageRank... ---\n",
      "--- ‚úÖ PageRank Calculated for 55 pages. ---\n",
      "\n",
      "--- PageRank Scores (Sample) ---\n",
      "  page_yq2gbbd1  : 0.022767\n",
      "  page_ox8yl94w  : 0.013298\n",
      "  page_j5vo7ivh  : 0.023040\n",
      "  page_k38nkc2n  : 0.032661\n",
      "  page_5ebo1o8u  : 0.015242\n",
      "  page_y58wfhk5  : 0.017040\n",
      "  page_rascybsa  : 0.011747\n",
      "  page_trtuhhex  : 0.023146\n",
      "  page_qen7f20c  : 0.021183\n",
      "  page_q0mdnv7q  : 0.003725\n",
      "  ... and so on\n",
      "\n",
      "--- Validation ---\n",
      "  Total score (should be ~1.0): 1.000000\n",
      "  Pages in graph:   55\n",
      "  Pages in PageRank: 55\n",
      "PageRank test finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# This patch is needed for Jupyter notebooks to run asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Test Step 3: PageRank Calculator ---\n",
    "# Goal: Run the crawler, then use its graph to calculate PageRank.\n",
    "\n",
    "try:\n",
    "    from crawler import Crawler\n",
    "    print(\"Successfully imported Crawler class from crawler.py\")\n",
    "    from pagerank import calculate_pagerank\n",
    "    print(\"Successfully imported calculate_pagerank from pagerank.py\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå FAILED TO IMPORT. Make sure 'crawler.py' and 'pagerank.py' are in the same folder.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "async def test_step_3_pagerank():\n",
    "    \"\"\"\n",
    "    Runs the crawler and then calculates PageRank.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üöÄ Running Crawler to get graph... ---\")\n",
    "    crawler = Crawler(start_page=\"/\") \n",
    "    await crawler.crawl()\n",
    "    \n",
    "    if not crawler.graph:\n",
    "        print(\"‚ùå TEST FAILED: Crawler returned an empty graph.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- ‚úÖ Crawler finished. Found {len(crawler.graph)} pages. ---\")\n",
    "    print(\"--- üöÄ Calculating PageRank... ---\")\n",
    "    \n",
    "    pagerank_scores = calculate_pagerank(crawler.graph)\n",
    "    \n",
    "    if not pagerank_scores:\n",
    "        print(\"‚ùå TEST FAILED: PageRank calculation returned no scores.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"--- ‚úÖ PageRank Calculated for {len(pagerank_scores)} pages. ---\")\n",
    "    \n",
    "    # --- Print a sample of the scores ---\n",
    "    print(\"\\n--- PageRank Scores (Sample) ---\")\n",
    "    total_score = 0.0\n",
    "    for i, (page, score) in enumerate(pagerank_scores.items()):\n",
    "        if i < 10: # Print first 10\n",
    "            print(f\"  {page:<15}: {score:.6f}\")\n",
    "        total_score += score\n",
    "    \n",
    "    if len(pagerank_scores) > 10:\n",
    "        print(\"  ... and so on\")\n",
    "        \n",
    "    print(\"\\n--- Validation ---\")\n",
    "    print(f\"  Total score (should be ~1.0): {total_score:.6f}\")\n",
    "    print(f\"  Pages in graph:   {len(crawler.graph)}\")\n",
    "    print(f\"  Pages in PageRank: {len(pagerank_scores)}\")\n",
    "\n",
    "\n",
    "# --- Run the test ---\n",
    "print(\"Starting PageRank test (includes full crawl)...\")\n",
    "asyncio.run(test_step_3_pagerank())\n",
    "print(\"PageRank test finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087bcf9",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a491f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported Crawler.\n",
      "Successfully imported calculate_pagerank.\n",
      "Successfully imported evaluator functions.\n",
      "Starting Full Pipeline test (Crawl, PageRank, Submit)...\n",
      "\n",
      "--- üöÄ [1/4] Running Crawler to get graph... ---\n",
      "--- üöÄ Starting crawl from / ---\n",
      "\n",
      "--- ‚úÖ Crawl Finished ---\n",
      "  Total pages found: 55\n",
      "  Total visits made: 56\n",
      "  Time taken:        1.33 seconds\n",
      "\n",
      "--- Node Data (Sample) ---\n",
      "  page_ondib3z5: yrlelpggirpt\n",
      "  page_ql618swi: a9y21zjt0crr\n",
      "  page_idxild28: z39371o0rkua\n",
      "  page_2x0nyoul: bdwtz31ug689\n",
      "  page_q0mdnv7q: dgmd3gmf82ee\n",
      "...\n",
      "\n",
      "--- ‚úÖ [1/4] Crawler finished. Found 55 pages. ---\n",
      "\n",
      "--- üöÄ [2/4] Calculating PageRank... ---\n",
      "--- ‚úÖ [2/4] PageRank Calculated. ---\n",
      "\n",
      "--- üöÄ [3/4] Formatting payload... ---\n",
      "--- ‚úÖ [3/4] Payload formatted with 55 entries. ---\n",
      "\n",
      "--- üöÄ [4/4] Submitting to /evaluate... ---\n",
      "Submit successful (HTTP 200).\n",
      "\n",
      "--- ‚úÖ [4/4] ...Submission Complete! ---\n",
      "\n",
      "==============================\n",
      " SERVER EVALUATION RESPONSE \n",
      "==============================\n",
      "{\n",
      "  \"avg_staleness\": 3647.018181818182,\n",
      "  \"coverage\": 0.9649122807017544,\n",
      "  \"covered_nodes\": 55,\n",
      "  \"matched_entries\": 55,\n",
      "  \"mse\": 1.0796476738396446e-07,\n",
      "  \"total_nodes\": 57,\n",
      "  \"visit_count\": 169\n",
      "}\n",
      "==============================\n",
      "\n",
      "üéâüéâüéâ TEST PASSED! We successfully submitted and got scored. üéâüéâüéâ\n",
      "Full Pipeline test finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "\n",
    "# This patch is needed for Jupyter notebooks to run asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Test Step 4: First Evaluation Submission ---\n",
    "# Goal: Run all components and submit to /evaluate once.\n",
    "\n",
    "try:\n",
    "    from crawler import Crawler\n",
    "    print(\"Successfully imported Crawler.\")\n",
    "    from pagerank import calculate_pagerank\n",
    "    print(\"Successfully imported calculate_pagerank.\")\n",
    "    from evaluator import format_payload, submit_evaluation\n",
    "    print(\"Successfully imported evaluator functions.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå FAILED TO IMPORT. Make sure all .py files are in the same folder.\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "async def test_step_4_submit():\n",
    "    \"\"\"\n",
    "    Runs the full pipeline: Crawl -> PageRank -> Format -> Submit\n",
    "    \"\"\"\n",
    "    print(\"\\n--- üöÄ [1/4] Running Crawler to get graph... ---\")\n",
    "    # Our crawler's first fetch to '/' starts the 60s timer\n",
    "    crawler = Crawler(start_page=\"/\") \n",
    "    await crawler.crawl()\n",
    "    \n",
    "    if not crawler.graph:\n",
    "        print(\"‚ùå TEST FAILED: Crawler returned an empty graph.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- ‚úÖ [1/4] Crawler finished. Found {len(crawler.graph)} pages. ---\")\n",
    "    \n",
    "    print(\"\\n--- üöÄ [2/4] Calculating PageRank... ---\")\n",
    "    pagerank_scores = calculate_pagerank(crawler.graph)\n",
    "    print(f\"--- ‚úÖ [2/4] PageRank Calculated. ---\")\n",
    "\n",
    "    print(\"\\n--- üöÄ [3/4] Formatting payload... ---\")\n",
    "    payload = format_payload(crawler.node_data, pagerank_scores)\n",
    "    print(f\"--- ‚úÖ [3/4] Payload formatted with {len(payload['entries'])} entries. ---\")\n",
    "\n",
    "    # Uncomment this line if you want to inspect the JSON\n",
    "    # print(json.dumps(payload, indent=2))\n",
    "\n",
    "    print(\"\\n--- üöÄ [4/4] Submitting to /evaluate... ---\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # The crawler already visited the site, so the timer has started\n",
    "        # and this submission is allowed.\n",
    "        submission_response = await submit_evaluation(session, payload)\n",
    "    \n",
    "    print(\"\\n--- ‚úÖ [4/4] ...Submission Complete! ---\")\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\" SERVER EVALUATION RESPONSE \")\n",
    "    print(\"=\"*30)\n",
    "    print(json.dumps(submission_response, indent=2))\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    if 'mse' in submission_response:\n",
    "        print(\"\\nüéâüéâüéâ TEST PASSED! We successfully submitted and got scored. üéâüéâüéâ\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå TEST FAILED. Server returned an error. Check the response above.\")\n",
    "\n",
    "\n",
    "# --- Run the test ---\n",
    "print(\"Starting Full Pipeline test (Crawl, PageRank, Submit)...\")\n",
    "asyncio.run(test_step_4_submit())\n",
    "print(\"Full Pipeline test finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cabdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
