{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6480c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   given_name     4844 non-null   object \n",
      " 1   surname        4921 non-null   object \n",
      " 2   street_number  4755 non-null   float64\n",
      " 3   address_1      4846 non-null   object \n",
      " 4   address_2      4307 non-null   object \n",
      " 5   suburb         4915 non-null   object \n",
      " 6   postcode       5000 non-null   int64  \n",
      " 7   state          4915 non-null   object \n",
      " 8   date_of_birth  4845 non-null   float64\n",
      " 9   soc_sec_id     5000 non-null   int64  \n",
      " 10  id             5000 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 429.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3689.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1418.253235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4670.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9396.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          postcode\n",
       "count  5000.000000\n",
       "mean   3689.369400\n",
       "std    1418.253235\n",
       "min     331.000000\n",
       "25%    2500.000000\n",
       "50%    3197.000000\n",
       "75%    4670.000000\n",
       "max    9396.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/dedup_data.csv\")\n",
    "df.info()\n",
    "df.head()\n",
    "df[['given_name','surname','postcode']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d58e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "given_name       156\n",
      "surname           79\n",
      "street_number    245\n",
      "address_1        154\n",
      "address_2        693\n",
      "suburb            85\n",
      "postcode           0\n",
      "state             85\n",
      "date_of_birth    155\n",
      "soc_sec_id         0\n",
      "id                 0\n",
      "dtype: int64\n",
      "Sample date_of_birth values:\n",
      "0    19560409.0\n",
      "1    19080419.0\n",
      "2    19081128.0\n",
      "3    19921119.0\n",
      "4    19991207.0\n",
      "Name: date_of_birth, dtype: float64\n",
      "\n",
      "Postcode length distribution:\n",
      "postcode\n",
      "4    4964\n",
      "3      36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "String column summary:\n",
      "       given_name surname              address_1       address_2     suburb  \\\n",
      "count        4844    4921                   4846            4307       4915   \n",
      "unique       1213    1740                   2358            2303       1706   \n",
      "top        joshua   white  newman morris circuit  brentwood vlge  frankston   \n",
      "freq           81     123                     18              32         44   \n",
      "\n",
      "       state  \n",
      "count   4915  \n",
      "unique    35  \n",
      "top      nsw  \n",
      "freq    1581  \n",
      "\n",
      "State values:\n",
      "state\n",
      "nsw    1581\n",
      "vic    1212\n",
      "qld     821\n",
      "wa      496\n",
      "sa      463\n",
      "tas     118\n",
      "act      96\n",
      "nt       57\n",
      "nws      13\n",
      "vci      11\n",
      "qdl       6\n",
      "nsq       5\n",
      "as        4\n",
      "qls       4\n",
      "aw        2\n",
      "ss        2\n",
      "qlf       2\n",
      "vid       2\n",
      "nxw       2\n",
      "nss       2\n",
      "tsa       2\n",
      "qle       1\n",
      "vix       1\n",
      "sic       1\n",
      "viv       1\n",
      "ws        1\n",
      "nsh       1\n",
      "nhw       1\n",
      "sct       1\n",
      "vkc       1\n",
      "nf        1\n",
      "nsy       1\n",
      "wq        1\n",
      "nse       1\n",
      "qkd       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"Sample date_of_birth values:\")\n",
    "print(df[df['date_of_birth'].notnull()]['date_of_birth'].head())\n",
    "\n",
    "print(\"\\nPostcode length distribution:\")\n",
    "print(df['postcode'].astype(str).str.len().value_counts())\n",
    "\n",
    "print(\"\\nString column summary:\")\n",
    "print(df.describe(include='object'))\n",
    "\n",
    "# Specifically check 'state' as it's a good blocking candidate\n",
    "print(\"\\nState values:\")\n",
    "print(df['state'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "debc7760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique soc_sec_id: 2291\n",
      "Most common soc_sec_ids (Top 10):\n",
      "soc_sec_id\n",
      "9305219    6\n",
      "4669127    6\n",
      "2661643    6\n",
      "7462490    6\n",
      "1677968    6\n",
      "5449129    6\n",
      "9687938    6\n",
      "8928429    6\n",
      "4282147    6\n",
      "2510937    6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total rows: 5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTotal unique soc_sec_id: {df['soc_sec_id'].nunique()}\")\n",
    "print(\"Most common soc_sec_ids (Top 10):\")\n",
    "print(df['soc_sec_id'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa432c0",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5491a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Data Info\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   given_name     5000 non-null   object\n",
      " 1   surname        5000 non-null   object\n",
      " 2   street_number  5000 non-null   object\n",
      " 3   address_1      5000 non-null   object\n",
      " 4   address_2      5000 non-null   object\n",
      " 5   suburb         5000 non-null   object\n",
      " 6   postcode       5000 non-null   object\n",
      " 7   state          5000 non-null   object\n",
      " 8   date_of_birth  5000 non-null   object\n",
      " 9   soc_sec_id     5000 non-null   int64 \n",
      " 10  id             5000 non-null   int64 \n",
      " 11  full_address   5000 non-null   object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 468.9+ KB\n",
      "\n",
      "Cleaned Data Head\n",
      "  given_name   surname street_number        address_1               address_2  \\\n",
      "0   mitchell     green             7    wallaby place                  delmar   \n",
      "1     harley  mccarthy           177    pridhamstreet                  milton   \n",
      "2   madeline     mason            54  hoseason street  lakefront retrmnt vlge   \n",
      "3   isabelle                      23    gundulu place               currin ga   \n",
      "4     taylor  hathaway             7   yuranigh court          brentwood vlge   \n",
      "\n",
      "      suburb postcode state date_of_birth  soc_sec_id     id  \\\n",
      "0  cleveland     2119    sa    1956-04-09     1804974  74463   \n",
      "1    marsden     3165   nsw    1908-04-19     6089216  60733   \n",
      "2  granville     4881   nsw    1908-11-28     2185997  64831   \n",
      "3   utakarra     2193    wa    1992-11-19     4314184  12416   \n",
      "4                4220   nsw    1999-12-07     9144092  81570   \n",
      "\n",
      "                                full_address  \n",
      "0                     7 wallaby place delmar  \n",
      "1                   177 pridhamstreet milton  \n",
      "2  54 hoseason street lakefront retrmnt vlge  \n",
      "3                 23 gundulu place currin ga  \n",
      "4            7 yuranigh court brentwood vlge  \n",
      "\n",
      "Cleaned State Values\n",
      "state\n",
      "nsw    1607\n",
      "vic    1228\n",
      "qld     836\n",
      "wa      499\n",
      "sa      470\n",
      "tas     120\n",
      "act      97\n",
      "         85\n",
      "nt       58\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataframe again (just to be safe)\n",
    "df = pd.read_csv(\"../data/dedup_data.csv\")\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Fix Data Types and Standardize\n",
    "\n",
    "# POSTCODE: Convert to string, pad with leading zeros to 4 digits\n",
    "# (Handles the '331' -> '0331' problem)\n",
    "df_clean['postcode'] = df_clean['postcode'].astype(str).str.zfill(4)\n",
    "\n",
    "# STREET NUMBER: Convert to string, fill NaNs, remove '.0'\n",
    "df_clean['street_number'] = df_clean['street_number'].fillna('').astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "# DATE_OF_BIRTH: Convert YYYYMMDD.0 float to 'YYYY-MM-DD' string\n",
    "# Use pd.to_datetime for robust conversion, handling errors\n",
    "df_clean['date_of_birth'] = pd.to_datetime(\n",
    "    df_clean['date_of_birth'].fillna(0).astype(int).astype(str),\n",
    "    format='%Y%m%d',\n",
    "    errors='coerce' # If any date is invalid (e.g., 0), make it NaT (Not a Time)\n",
    ")\n",
    "# Now convert the valid dates to a standard string format\n",
    "df_clean['date_of_birth'] = df_clean['date_of_birth'].dt.strftime('%Y-%m-%d').fillna('')\n",
    "\n",
    "\n",
    "# 2. Clean and Normalize String Columns\n",
    "\n",
    "# Fill NaNs in all object columns with an empty string\n",
    "str_cols = df_clean.select_dtypes(include='object').columns\n",
    "df_clean[str_cols] = df_clean[str_cols].fillna('')\n",
    "\n",
    "# Normalize all string columns: lowercase and strip whitespace\n",
    "for col in str_cols:\n",
    "    if col != 'postcode': # We already processed postcode\n",
    "        df_clean[col] = df_clean[col].str.lower().str.strip()\n",
    "\n",
    "# STATE: Manually fix the typos we found in the EDA\n",
    "state_map = {\n",
    "    'nws': 'nsw', 'nsq': 'nsw', 'nxw': 'nsw', 'nss': 'nsw', \n",
    "    'nsh': 'nsw', 'nhw': 'nsw', 'nsy': 'nsw', 'nse': 'nsw',\n",
    "    'vci': 'vic', 'vid': 'vic', 'vix': 'vic', 'viv': 'vic', 'vkc': 'vic',\n",
    "    'qdl': 'qld', 'qls': 'qld', 'qlf': 'qld', 'qle': 'qld', 'qkd': 'qld', 'wq': 'qld',\n",
    "    'aw': 'wa', 'ws': 'wa',\n",
    "    'as': 'sa', 'ss': 'sa', 'sic': 'sa',\n",
    "    'tsa': 'tas',\n",
    "    'sct': 'act',\n",
    "    'nf': 'nt' # Assuming nf is northern territory? Or maybe norfolk island. Safe to map to nt for now.\n",
    "}\n",
    "df_clean['state'] = df_clean['state'].replace(state_map)\n",
    "\n",
    "\n",
    "# 3. Create Composite Features for Matching\n",
    "\n",
    "# Create a clean, combined address field\n",
    "df_clean['full_address'] = (\n",
    "    df_clean['street_number'] + ' ' + \n",
    "    df_clean['address_1'] + ' ' + \n",
    "    df_clean['address_2']\n",
    ")\n",
    "# Clean up extra spaces\n",
    "df_clean['full_address'] = df_clean['full_address'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "# 4. Final Check\n",
    "\n",
    "print(\"\\nCleaned Data Info\")\n",
    "df_clean.info()\n",
    "\n",
    "print(\"\\nCleaned Data Head\")\n",
    "print(df_clean.head())\n",
    "\n",
    "print(\"\\nCleaned State Values\")\n",
    "print(df_clean['state'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3b05a",
   "metadata": {},
   "source": [
    "# Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1fa8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate 'id' values: 127\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicate 'id' values: {df_clean['id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae99a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocking Results\n",
      "Total number of records: 5000\n",
      "Number of pairs WITHOUT blocking (calculated): 12497500\n",
      "Number of pairs WITH blocking on 'postcode': 16115\n",
      "Pairwise comparisons reduced by: 99.87%\n"
     ]
    }
   ],
   "source": [
    "import recordlinkage\n",
    "\n",
    "# 1. Create a new indexer for blocking\n",
    "indexer_blocked = recordlinkage.Index()\n",
    "\n",
    "# 2. Define the blocking rule: only match records in the same 'postcode'\n",
    "indexer_blocked.block('postcode')\n",
    "\n",
    "# 3. Generate the candidate pairs using df_clean directly.\n",
    "# It will use its default unique index (0-4999).\n",
    "# The 'id' column remains as a normal data column.\n",
    "candidate_pairs = indexer_blocked.index(df_clean)\n",
    "\n",
    "# 4. Show the results\n",
    "print(\"Blocking Results\")\n",
    "\n",
    "total_records = len(df_clean)\n",
    "# Calculate the full pairs count manually\n",
    "full_pairs_count = (total_records * (total_records - 1)) / 2\n",
    "\n",
    "print(f\"Total number of records: {total_records}\")\n",
    "print(f\"Number of pairs WITHOUT blocking (calculated): {int(full_pairs_count)}\")\n",
    "print(f\"Number of pairs WITH blocking on 'postcode': {len(candidate_pairs)}\")\n",
    "\n",
    "reduction = (1 - (len(candidate_pairs) / full_pairs_count)) * 100\n",
    "print(f\"Pairwise comparisons reduced by: {reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70d2b4",
   "metadata": {},
   "source": [
    "# Pairwise Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a78d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pairwise comparison for 16,115 pairs...\n",
      "\n",
      "--- Comparison Feature Vectors (Head) ---\n",
      "       given_name_sim  surname_sim  dob_sim  address_sim  state_match\n",
      "31 30             1.0          1.0      1.0          1.0            1\n",
      "42 17             1.0          1.0      1.0          1.0            1\n",
      "46 35             1.0          1.0      1.0          1.0            1\n",
      "59 27             1.0          1.0      1.0          1.0            1\n",
      "71 1              0.0          0.0      0.0          0.0            0\n",
      "\n",
      "--- Summary of Matches Found ---\n",
      "given_name_sim    3724.0\n",
      "surname_sim       3983.0\n",
      "dob_sim           4600.0\n",
      "address_sim       4142.0\n",
      "state_match       6737.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import recordlinkage\n",
    "import jellyfish # Make sure this is installed\n",
    "\n",
    "# We need df_clean and candidate_pairs from the previous steps\n",
    "\n",
    "print(\"Starting pairwise comparison for 16,115 pairs...\")\n",
    "\n",
    "# 1. Create the comparison object\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "# 2. Define the comparison rules\n",
    "# String similarity (Jaro-Winkler) for noisy fields.\n",
    "# Score 1.0 = perfect match, 0.0 = no match.\n",
    "compare_cl.string('given_name', 'given_name', method='jarowinkler', threshold=0.85, label='given_name_sim')\n",
    "compare_cl.string('surname', 'surname', method='jarowinkler', threshold=0.85, label='surname_sim')\n",
    "compare_cl.string('date_of_birth', 'date_of_birth', method='jarowinkler', threshold=0.9, label='dob_sim')\n",
    "compare_cl.string('full_address', 'full_address', method='jarowinkler', threshold=0.85, label='address_sim')\n",
    "\n",
    "# Exact match for the 'state' field we cleaned\n",
    "compare_cl.exact('state', 'state', label='state_match')\n",
    "\n",
    "# 3. Compute the similarities for all candidate pairs\n",
    "# This will return a DataFrame with your 16,115 pairs\n",
    "# and the 5 similarity scores for each.\n",
    "feature_vectors = compare_cl.compute(candidate_pairs, df_clean)\n",
    "\n",
    "# 4. Show the results\n",
    "print(\"\\n--- Comparison Feature Vectors (Head) ---\")\n",
    "print(feature_vectors.head())\n",
    "\n",
    "print(\"\\n--- Summary of Matches Found ---\")\n",
    "print(feature_vectors.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0791a2",
   "metadata": {},
   "source": [
    "# Find Matches and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f8d1f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match Score Distribution\n",
      "5.0    2216\n",
      "4.0    1731\n",
      "3.0     834\n",
      "2.0     216\n",
      "1.0    2248\n",
      "0.0    8870\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Found 3947 matching pairs (score >= 4.0)\n",
      "Total number of clusters (people) found: 1059\n",
      "Total number of records: 5000\n",
      "\n",
      "Sample Clusters (Row IDs)\n",
      "Cluster 1: {3872, 4333, 30, 31}\n",
      "Cluster 2: {17, 42}\n",
      "Cluster 3: {35, 46, 820, 2773, 600, 1210}\n",
      "Cluster 4: {27, 59, 1535}\n",
      "Cluster 5: {226, 138, 49, 2774, 2334}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# feature_vectors is the DataFrame from your previous step\n",
    "\n",
    "# Step 5: Decision Rule\n",
    "\n",
    "# 1. Create a total 'match_score' by summing the 5 features\n",
    "match_score = feature_vectors.sum(axis=1)\n",
    "\n",
    "# 2. Let's look at the distribution of scores\n",
    "# This helps us pick a good threshold.\n",
    "print(\"Match Score Distribution\")\n",
    "print(match_score.value_counts().sort_index(ascending=False))\n",
    "\n",
    "# 3. Set your threshold. A score of 4.0 or 5.0 is a very strong match.\n",
    "# A score of 3.0 might also be a match. Let's start with 4.0\n",
    "MATCH_THRESHOLD = 4.0 \n",
    "\n",
    "# 4. Get all pairs that meet or exceed this threshold\n",
    "matches = feature_vectors[match_score >= MATCH_THRESHOLD]\n",
    "matching_pairs_list = matches.index.to_list()\n",
    "\n",
    "print(f\"\\nFound {len(matching_pairs_list)} matching pairs (score >= {MATCH_THRESHOLD})\")\n",
    "\n",
    "# Step 6: Clustering\n",
    "\n",
    "# 1. Create a graph from the list of matching pairs\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(matching_pairs_list)\n",
    "\n",
    "# 2. Find all connected components (the clusters)\n",
    "clusters = list(nx.connected_components(G))\n",
    "\n",
    "# 3. Show Final Results\n",
    "print(f\"Total number of clusters (people) found: {len(clusters)}\")\n",
    "print(f\"Total number of records: {len(df_clean)}\")\n",
    "\n",
    "# Let's look at the first 5 clusters\n",
    "print(\"\\nSample Clusters (Row IDs)\")\n",
    "for i, cluster in enumerate(clusters[:5]):\n",
    "    print(f\"Cluster {i+1}: {cluster}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce5715",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20c9d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Cluster Performance\n",
      "\n",
      "Ground Truth (soc_sec_id) Clusters: 2291\n",
      "Algorithm (Predicted) Clusters: 1059\n",
      "\n",
      "Adjusted Rand Score (ARS): 0.7307\n",
      "Result: Great! Your model captured most of the groups correctly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/metrics/cluster/_supervised.py:50: UserWarning: The number of unique classes is greater than 50% of the number of samples. `y` could represent a regression problem, not a classification problem.\n",
      "  type_pred = type_of_target(labels_pred)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd\n",
    "\n",
    "# 'clusters' is the list of clusters from your previous step\n",
    "# 'df_clean' is your cleaned DataFrame\n",
    "\n",
    "print(\"Evaluating Cluster Performance\")\n",
    "\n",
    "# 1. Create a \"predicted labels\" array\n",
    "# First, create a mapping from row_id -> cluster_id\n",
    "cluster_mapping = {}\n",
    "for cluster_id, row_ids in enumerate(clusters):\n",
    "    for row_id in row_ids:\n",
    "        cluster_mapping[row_id] = cluster_id\n",
    "\n",
    "# Now, create the 'predictions' array for all 5000 records\n",
    "# We must ensure it's in the same order (0-4999) as df_clean\n",
    "predictions = []\n",
    "for row_id in range(len(df_clean)):\n",
    "    # If a record was a singleton (not in any cluster),\n",
    "    # give it a unique negative cluster_id\n",
    "    predictions.append(cluster_mapping.get(row_id, -row_id - 1))\n",
    "\n",
    "# 2. Get the \"true labels\" array\n",
    "# This is just the 'soc_sec_id' column\n",
    "truth = df_clean['soc_sec_id'].to_list()\n",
    "\n",
    "# 3. Calculate and print the score\n",
    "ars_score = adjusted_rand_score(truth, predictions)\n",
    "\n",
    "print(f\"\\nGround Truth (soc_sec_id) Clusters: {df_clean['soc_sec_id'].nunique()}\")\n",
    "print(f\"Algorithm (Predicted) Clusters: {len(clusters)}\")\n",
    "print(f\"\\nAdjusted Rand Score (ARS): {ars_score:.4f}\")\n",
    "\n",
    "if ars_score > 0.9:\n",
    "    print(\"Result: Excellent! Your model's clusters are a near-perfect match.\")\n",
    "elif ars_score > 0.7:\n",
    "    print(\"Result: Great! Your model captured most of the groups correctly.\")\n",
    "else:\n",
    "    print(\"Result: Good. The model found a strong signal but could be tuned.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
