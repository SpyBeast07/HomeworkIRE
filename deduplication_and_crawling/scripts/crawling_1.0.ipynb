{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473bf557",
   "metadata": {},
   "source": [
    "# Ping the Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84a4f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinging http://localhost:3000...\n",
      "\n",
      "--- Raw Server Response Text ---\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Crawling Assignment  - Main Portal</title>\n",
      "    <style>\n",
      "        * {\n",
      "            margin: 0;\n",
      "            padding: 0;\n",
      "            box-sizing: border-box;\n",
      "        }\n",
      "        \n",
      "        body {\n",
      "            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;\n",
      "            background: #f5f7fa;\n",
      "            min-height: 100vh;\n",
      "            color: #2c3e50;\n",
      "            line-height: 1.6;\n",
      "        }\n",
      "        \n",
      "        .header {\n",
      "            background: #34495e;\n",
      "            color: white;\n",
      "            padding: 1rem 0;\n",
      "            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
      "        }\n",
      "        \n",
      "        .header-content {\n",
      "            max-width: 1200px;\n",
      "            margin: 0 auto;\n",
      "            padding: 0 2rem;\n",
      "            display: flex;\n",
      "            justify-content: space-between;\n",
      "            align-items: center;\n",
      "        }\n",
      "        \n",
      "        .header h1 {\n",
      "            font-size: 1.8rem;\n",
      "            font-weight: 600;\n",
      "        }\n",
      "        \n",
      "        .page-id {\n",
      "            background: #3498db;\n",
      "            padding: 0.4rem 0.8rem;\n",
      "            border-radius: 15px;\n",
      "            font-size: 0.9rem;\n",
      "            font-weight: 500;\n",
      "        }\n",
      "        \n",
      "        .container {\n",
      "            max-width: 1200px;\n",
      "            margin: 0 auto;\n",
      "            padding: 2rem;\n",
      "        }\n",
      "        \n",
      "        .breadcrumb {\n",
      "            background: white;\n",
      "            padding: 1rem;\n",
      "            border-radius: 8px;\n",
      "            margin-bottom: 2rem;\n",
      "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
      "            font-size: 0.9rem;\n",
      "            color: #7f8c8d;\n",
      "        }\n",
      "        \n",
      "        .section {\n",
      "            background: white;\n",
      "            border-radius: 8px;\n",
      "            padding: 2rem;\n",
      "            margin-bottom: 2rem;\n",
      "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
      "        }\n",
      "        \n",
      "        .section h2 {\n",
      "            color: #2c3e50;\n",
      "            font-size: 1.5rem;\n",
      "            margin-bottom: 1.5rem;\n",
      "            padding-bottom: 0.5rem;\n",
      "            border-bottom: 2px solid #ecf0f1;\n",
      "        }\n",
      "        \n",
      "        .files-table {\n",
      "            width: 100%;\n",
      "            border-collapse: collapse;\n",
      "            margin-top: 1rem;\n",
      "        }\n",
      "        \n",
      "        .files-table th,\n",
      "        .files-table td {\n",
      "            padding: 1rem;\n",
      "            text-align: left;\n",
      "            border-bottom: 1px solid #ecf0f1;\n",
      "        }\n",
      "        \n",
      "        .files-table th {\n",
      "            background: #f8f9fa;\n",
      "            font-weight: 600;\n",
      "            color: #2c3e50;\n",
      "        }\n",
      "        \n",
      "        .files-table tr:hover {\n",
      "            background: #f8f9fa;\n",
      "        }\n",
      "        \n",
      "        .file-icon {\n",
      "            font-size: 1.2rem;\n",
      "            margin-right: 0.5rem;\n",
      "        }\n",
      "        \n",
      "        .file-link {\n",
      "            color: #3498db;\n",
      "            text-decoration: none;\n",
      "            font-weight: 500;\n",
      "        }\n",
      "        \n",
      "        .file-link:hover {\n",
      "            color: #2980b9;\n",
      "            text-decoration: underline;\n",
      "        }\n",
      "        \n",
      "        .file-size {\n",
      "            color: #7f8c8d;\n",
      "            font-size: 0.9rem;\n",
      "        }\n",
      "        \n",
      "        .navigation-links {\n",
      "            display: grid;\n",
      "            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
      "            gap: 1rem;\n",
      "            margin-top: 1rem;\n",
      "        }\n",
      "        \n",
      "        .nav-link {\n",
      "            display: block;\n",
      "            background: #ecf0f1;\n",
      "            padding: 1rem;\n",
      "            border-radius: 6px;\n",
      "            text-decoration: none;\n",
      "            color: #2c3e50;\n",
      "            transition: all 0.2s ease;\n",
      "            border-left: 4px solid #3498db;\n",
      "        }\n",
      "        \n",
      "        .nav-link:hover {\n",
      "            background: #d5dbdb;\n",
      "            transform: translateX(2px);\n",
      "        }\n",
      "        \n",
      "        .nav-link-title {\n",
      "            font-weight: 600;\n",
      "            margin-bottom: 0.3rem;\n",
      "        }\n",
      "        \n",
      "        .nav-link-desc {\n",
      "            font-size: 0.9rem;\n",
      "            color: #7f8c8d;\n",
      "        }\n",
      "        \n",
      "        .empty-state {\n",
      "            text-align: center;\n",
      "            padding: 3rem;\n",
      "            color: #95a5a6;\n",
      "        }\n",
      "        \n",
      "        .empty-state-icon {\n",
      "            font-size: 3rem;\n",
      "            margin-bottom: 1rem;\n",
      "        }\n",
      "        \n",
      "        .stats {\n",
      "            display: grid;\n",
      "            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n",
      "            gap: 1rem;\n",
      "            margin-bottom: 2rem;\n",
      "        }\n",
      "        \n",
      "        .stat-card {\n",
      "            background: white;\n",
      "            padding: 1.5rem;\n",
      "            border-radius: 8px;\n",
      "            box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n",
      "            text-align: center;\n",
      "        }\n",
      "        \n",
      "        .stat-number {\n",
      "            font-size: 2rem;\n",
      "            font-weight: bold;\n",
      "            color: #3498db;\n",
      "            margin-bottom: 0.5rem;\n",
      "        }\n",
      "        \n",
      "        .stat-label {\n",
      "            color: #7f8c8d;\n",
      "            font-size: 0.9rem;\n",
      "        }\n",
      "        \n",
      "        .footer {\n",
      "            background: #34495e;\n",
      "            color: white;\n",
      "            text-align: center;\n",
      "            padding: 2rem 0;\n",
      "            margin-top: 3rem;\n",
      "        }\n",
      "        \n",
      "        .footer p {\n",
      "            margin: 0.5rem 0;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"header\">\n",
      "        <div class=\"header-content\">\n",
      "            <h1>Assignment 2: Crawling</h1>\n",
      "            <div class=\"page-id\">Page ID: page_2xq0nsn7</div>\n",
      "        </div>\n",
      "    </div>\n",
      "    \n",
      "    <div class=\"container\">\n",
      "        <div class=\"breadcrumb\">üè† Home / Main Portal</div>\n",
      "        \n",
      "        <div class=\"stats\">\n",
      "            <div class=\"stat-card\">\n",
      "                <div class=\"stat-number\">5</div>\n",
      "                <div class=\"stat-label\">Total Items</div>\n",
      "            </div>\n",
      "            <div class=\"stat-card\">\n",
      "                <div class=\"stat-number\">5</div>\n",
      "                <div class=\"stat-label\">Connected Pages</div>\n",
      "            </div>\n",
      "            <div class=\"stat-card\">\n",
      "                <div class=\"stat-number\">1</div>\n",
      "                <div class=\"stat-label\">Total Visits</div>\n",
      "            </div>\n",
      "        </div>\n",
      "        \n",
      "        <div class=\"section\">\n",
      "            <h2>Outgoing Links</h2>\n",
      "            <div style=\"margin-bottom:1.5rem;\">\n",
      "                    <span class=\"node-id\">Node ID: <b>j5p246evofgv</b></span><br>\n",
      "                    <span class=\"last-updated\" style=\"color: #7f8c8d; font-size: 0.9rem;\">Last Updated: 2025-11-07 17:22:37 UTC</span>\n",
      "                    \n",
      "                </div>\n",
      "                <table class=\"files-table\">\n",
      "                    <thead>\n",
      "                        <tr>\n",
      "                            <th>Name</th>\n",
      "                            <th>Actions</th>\n",
      "                        </tr>\n",
      "                    </thead>\n",
      "                    <tbody>\n",
      "                        \n",
      "                        <tr>\n",
      "                            <td>\n",
      "                                <span class=\"file-icon\">üìÅ</span>\n",
      "                                <span class=\"file-name\">page_0lfz4eyh/</span>\n",
      "                            </td>\n",
      "                            <td>\n",
      "                                <a href=\"/page_0lfz4eyh\" class=\"file-link\">Go</a>\n",
      "                            </td>\n",
      "                        </tr>\n",
      "                        \n",
      "                        <tr>\n",
      "                            <td>\n",
      "                                <span class=\"file-icon\">üìÅ</span>\n",
      "                                <span class=\"file-name\">page_qkqgewn3/</span>\n",
      "                            </td>\n",
      "                            <td>\n",
      "                                <a href=\"/page_qkqgewn3\" class=\"file-link\">Go</a>\n",
      "                            </td>\n",
      "                        </tr>\n",
      "                        \n",
      "                        <tr>\n",
      "                            <td>\n",
      "                                <span class=\"file-icon\">üìÅ</span>\n",
      "                                <span class=\"file-name\">page_9380fo98/</span>\n",
      "                            </td>\n",
      "                            <td>\n",
      "                                <a href=\"/page_9380fo98\" class=\"file-link\">Go</a>\n",
      "                            </td>\n",
      "                        </tr>\n",
      "                        \n",
      "                        <tr>\n",
      "                            <td>\n",
      "                                <span class=\"file-icon\">üìÅ</span>\n",
      "                                <span class=\"file-name\">page_juthp0u7/</span>\n",
      "                            </td>\n",
      "                            <td>\n",
      "                                <a href=\"/page_juthp0u7\" class=\"file-link\">Go</a>\n",
      "                            </td>\n",
      "                        </tr>\n",
      "                        \n",
      "                        <tr>\n",
      "                            <td>\n",
      "                                <span class=\"file-icon\">üìÅ</span>\n",
      "                                <span class=\"file-name\">page_5ne7xos4/</span>\n",
      "                            </td>\n",
      "                            <td>\n",
      "                                <a href=\"/page_5ne7xos4\" class=\"file-link\">Go</a>\n",
      "                            </td>\n",
      "                        </tr>\n",
      "                        \n",
      "                    </tbody>\n",
      "                </table>\n",
      "                    \n",
      "        </div>\n",
      "    </div>\n",
      "    \n",
      "    <div class=\"footer\">\n",
      "        <p>Assignment 2: Crawling</p>\n",
      "    </div>\n",
      "</body>\n",
      "</html>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "START_URL = \"http://localhost:3000\"\n",
    "\n",
    "print(f\"Pinging {START_URL}...\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(START_URL)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    print(\"\\n--- Raw Server Response Text ---\")\n",
    "    print(response.text)\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"\\nHTTP Error: {e}\")\n",
    "    print(\"This might be a '404 Not Found', which is useful!\")\n",
    "    print(\"\\n--- Raw Server Response Text (from error) ---\")\n",
    "    print(e.response.text)\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\"Error: Could not connect to {START_URL}\")\n",
    "    print(\"Please double-check that the Docker container is running.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f2baef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HTML Parsing Successful ---\n",
      "Start Page (Path): /\n",
      "Page ID: page_2xq0nsn7\n",
      "Node ID: j5p246evofgv\n",
      "Outgoing Links: ['/page_0lfz4eyh', '/page_qkqgewn3', '/page_9380fo98', '/page_juthp0u7', '/page_5ne7xos4']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re # We'll use regex for the node_id\n",
    "\n",
    "BASE_URL = \"http://localhost:3000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(BASE_URL)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # --- This is where we parse ---\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 1. Find the Page ID\n",
    "    page_id_tag = soup.find('div', class_='page-id')\n",
    "    # The text is \"Page ID: page_y5alujtp\", so we split it\n",
    "    page_id = page_id_tag.text.split(':')[-1].strip()\n",
    "    \n",
    "    # 2. Find the Node ID\n",
    "    node_id_tag = soup.find('span', class_='node-id')\n",
    "    # The text is \"Node ID: <b>7er2elqm01lj</b>\", so we find the <b> tag\n",
    "    node_id = node_id_tag.find('b').text.strip()\n",
    "    \n",
    "    # 3. Find all Outgoing Links\n",
    "    outgoing_links = []\n",
    "    # Find all <a> tags with the class 'file-link'\n",
    "    link_tags = soup.find_all('a', class_='file-link')\n",
    "    for tag in link_tags:\n",
    "        # Get the 'href' attribute, which is the link\n",
    "        outgoing_links.append(tag['href'])\n",
    "        \n",
    "    print(\"--- HTML Parsing Successful ---\")\n",
    "    print(f\"Start Page (Path): /\")\n",
    "    print(f\"Page ID: {page_id}\")\n",
    "    print(f\"Node ID: {node_id}\")\n",
    "    print(f\"Outgoing Links: {outgoing_links}\")\n",
    "    \n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(f\"Error: Could not connect to {BASE_URL}\")\n",
    "    print(\"Please ensure the Docker container is running.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during parsing: {e}\")\n",
    "    print(\"The HTML structure might have changed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e8fda",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de554752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Initial Crawl (BFS) ---\n",
      "Crawling: /\n",
      "Crawling: /page_0lfz4eyh\n",
      "Crawling: /page_qkqgewn3\n",
      "Crawling: /page_9380fo98\n",
      "Crawling: /page_juthp0u7\n",
      "Crawling: /page_5ne7xos4\n",
      "Crawling: /page_2xq0nsn7\n",
      "Crawling: /page_rbxstjjl\n",
      "Crawling: /page_vfzvzhyx\n",
      "Crawling: /page_e9u26xeo\n",
      "Crawling: /page_hxvqnxxz\n",
      "Crawling: /page_pz3yh635\n",
      "Crawling: /page_dndqper7\n",
      "Crawling: /page_tzj68wez\n",
      "\n",
      "--- Crawl Complete ---\n",
      "Total unique pages discovered: 14\n",
      "\n",
      "--- Discovered Page Graph (First 3 Items) ---\n",
      "{\n",
      "  \"/\": {\n",
      "    \"page_id\": \"page_2xq0nsn7\",\n",
      "    \"node_id\": \"j5p246evofgv\",\n",
      "    \"outgoing_links\": [\n",
      "      \"/page_0lfz4eyh\",\n",
      "      \"/page_qkqgewn3\",\n",
      "      \"/page_9380fo98\",\n",
      "      \"/page_juthp0u7\",\n",
      "      \"/page_5ne7xos4\"\n",
      "    ],\n",
      "    \"history\": []\n",
      "  },\n",
      "  \"/page_0lfz4eyh\": {\n",
      "    \"page_id\": \"page_0lfz4eyh\",\n",
      "    \"node_id\": \"yfcwvfusn3ci\",\n",
      "    \"outgoing_links\": [\n",
      "      \"/page_2xq0nsn7\",\n",
      "      \"/page_rbxstjjl\",\n",
      "      \"/page_vfzvzhyx\",\n",
      "      \"/page_5ne7xos4\"\n",
      "    ],\n",
      "    \"history\": []\n",
      "  },\n",
      "  \"/page_qkqgewn3\": {\n",
      "    \"page_id\": \"page_qkqgewn3\",\n",
      "    \"node_id\": \"ecut9e1oe3r6\",\n",
      "    \"outgoing_links\": [\n",
      "      \"/page_e9u26xeo\",\n",
      "      \"/page_vfzvzhyx\",\n",
      "      \"/page_hxvqnxxz\",\n",
      "      \"/page_rbxstjjl\"\n",
      "    ],\n",
      "    \"history\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "BASE_URL = \"http://localhost:3000\"\n",
    "\n",
    "def parse_page(html_content):\n",
    "    \"\"\"\n",
    "    Parses the HTML of a page and extracts all required data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # 1. Find Page ID\n",
    "        page_id_tag = soup.find('div', class_='page-id')\n",
    "        page_id = page_id_tag.text.split(':')[-1].strip()\n",
    "        \n",
    "        # 2. Find Node ID\n",
    "        node_id_tag = soup.find('span', class_='node-id')\n",
    "        node_id = node_id_tag.find('b').text.strip()\n",
    "        \n",
    "        # 3. Find Outgoing Links\n",
    "        outgoing_links = []\n",
    "        link_tags = soup.find_all('a', class_='file-link')\n",
    "        for tag in link_tags:\n",
    "            outgoing_links.append(tag['href'])\n",
    "            \n",
    "        # 4. Find Node ID History\n",
    "        history = []\n",
    "        details_tag = soup.find('details')\n",
    "        if details_tag:\n",
    "            # Find all <div>s with the style 'margin-left: 1rem...'\n",
    "            history_divs = details_tag.find_all('div', style=re.compile(r'margin-left'))\n",
    "            for div in history_divs:\n",
    "                # Text is '‚Ä¢ p5zg2ka84j0e (2025-11-07 16:48:53 UTC)'\n",
    "                text = div.text.strip('‚Ä¢ ')\n",
    "                match = re.search(r'^(.*?) \\((.*? UTC)\\)$', text)\n",
    "                if match:\n",
    "                    prev_node_id = match.group(1).strip()\n",
    "                    timestamp = match.group(2).strip()\n",
    "                    history.append({'node_id': prev_node_id, 'timestamp': timestamp})\n",
    "\n",
    "        return {\n",
    "            'page_id': page_id,\n",
    "            'node_id': node_id,\n",
    "            'outgoing_links': outgoing_links,\n",
    "            'history': history\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing page: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Crawler Logic ---\n",
    "\n",
    "# 1. Data Structures\n",
    "pages_to_visit = deque(['/'])  # Start at the root\n",
    "visited_pages = set()          # Keep track of where we've been\n",
    "page_graph = {}                # This is our final result\n",
    "\n",
    "print(\"--- Starting Initial Crawl (BFS) ---\")\n",
    "\n",
    "while pages_to_visit:\n",
    "    current_path = pages_to_visit.popleft() # Get the next page from the queue\n",
    "    \n",
    "    if current_path in visited_pages:\n",
    "        continue # Skip if we've already been here\n",
    "        \n",
    "    # Mark as visited\n",
    "    visited_pages.add(current_path)\n",
    "    print(f\"Crawling: {current_path}\")\n",
    "\n",
    "    # 2. Fetch the page\n",
    "    try:\n",
    "        url = BASE_URL + current_path\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 3. Parse the page\n",
    "        page_data = parse_page(response.text)\n",
    "        \n",
    "        if page_data:\n",
    "            # 4. Store the data\n",
    "            page_graph[current_path] = page_data\n",
    "            \n",
    "            # 5. Add new, unvisited links to the queue\n",
    "            for link in page_data['outgoing_links']:\n",
    "                if link not in visited_pages:\n",
    "                    pages_to_visit.append(link)\n",
    "                    \n",
    "        # Be a polite crawler (even locally)\n",
    "        time.sleep(0.05) \n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error crawling {current_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unknown error occurred at {current_path}: {e}\")\n",
    "\n",
    "# --- Crawl is Complete ---\n",
    "print(\"\\n--- Crawl Complete ---\")\n",
    "print(f\"Total unique pages discovered: {len(page_graph)}\")\n",
    "\n",
    "print(\"\\n--- Discovered Page Graph (First 3 Items) ---\")\n",
    "# Print the first 3 items from the graph\n",
    "preview = {k: page_graph[k] for k in list(page_graph.keys())[:3]}\n",
    "print(json.dumps(preview, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421863c",
   "metadata": {},
   "source": [
    "# Estimate PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67c7371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating PageRank ---\n",
      "\n",
      "--- PageRank Results (Sorted) ---\n",
      "/page_hxvqnxxz       | Score: 0.1203\n",
      "/page_5ne7xos4       | Score: 0.1180\n",
      "/page_juthp0u7       | Score: 0.1162\n",
      "/page_2xq0nsn7       | Score: 0.0822\n",
      "/page_rbxstjjl       | Score: 0.0780\n",
      "/page_0lfz4eyh       | Score: 0.0777\n",
      "/page_qkqgewn3       | Score: 0.0767\n",
      "/page_dndqper7       | Score: 0.0733\n",
      "/page_tzj68wez       | Score: 0.0726\n",
      "/page_vfzvzhyx       | Score: 0.0615\n",
      "/page_pz3yh635       | Score: 0.0594\n",
      "/page_e9u26xeo       | Score: 0.0270\n",
      "/page_9380fo98       | Score: 0.0265\n",
      "/                    | Score: 0.0107\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "# --- This script assumes 'page_graph' already exists in memory ---\n",
    "# Do NOT copy/paste the example dictionary.\n",
    "\n",
    "print(\"--- Calculating PageRank ---\")\n",
    "\n",
    "# 1. Check if the page_graph variable exists and is not empty\n",
    "if 'page_graph' not in locals() or not page_graph:\n",
    "    print(\"Error: 'page_graph' is not in memory or is empty.\")\n",
    "    print(\"Please re-run your crawler script first.\")\n",
    "else:\n",
    "    # 2. Create a new Directed Graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # 3. Build the graph from your 'page_graph' dictionary\n",
    "    for page_path, page_data in page_graph.items():\n",
    "        # Add the page as a node\n",
    "        G.add_node(page_path)\n",
    "        \n",
    "        # Add a directed edge for each outgoing link\n",
    "        for link in page_data['outgoing_links']:\n",
    "            # Ensure the link is also a node (for pages that might only be linked to)\n",
    "            if link not in G:\n",
    "                G.add_node(link) \n",
    "            G.add_edge(page_path, link)\n",
    "\n",
    "    # 4. Calculate PageRank\n",
    "    # This returns a dictionary: {page_path: pagerank_score, ...}\n",
    "    pagerank_scores = nx.pagerank(G)\n",
    "\n",
    "    # 5. Sort the results for easy viewing\n",
    "    sorted_pagerank = sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print(\"\\n--- PageRank Results (Sorted) ---\")\n",
    "    for page, score in sorted_pagerank:\n",
    "        print(f\"{page:<20} | Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1f2e9",
   "metadata": {},
   "source": [
    "# Estimate Change Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6ede63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Node ID Change Frequency ---\n",
      "\n",
      "--- Page Analysis Complete ---\n",
      "         page_path  pagerank  avg_interval_sec  last_node_id  freshness_score  pagerank_norm  freshness_norm  revisit_priority\n",
      "0                /  0.010714               NaN  j5p246evofgv              NaN       0.000000             NaN               NaN\n",
      "1   /page_0lfz4eyh  0.077657               NaN  yfcwvfusn3ci              NaN       0.610616             NaN               NaN\n",
      "2   /page_qkqgewn3  0.076651               NaN  ecut9e1oe3r6              NaN       0.601440             NaN               NaN\n",
      "3   /page_9380fo98  0.026510               NaN  fpbebh02zhi0              NaN       0.144079             NaN               NaN\n",
      "4   /page_juthp0u7  0.116154               NaN  mzsb3dkawtnf              NaN       0.961764             NaN               NaN\n",
      "5   /page_5ne7xos4  0.117979               NaN  3aqmqch2ib3t              NaN       0.978413             NaN               NaN\n",
      "6   /page_2xq0nsn7  0.082201               NaN  j5p246evofgv              NaN       0.652061             NaN               NaN\n",
      "7   /page_rbxstjjl  0.078031               NaN  a2h4m9m3q6f7              NaN       0.614034             NaN               NaN\n",
      "8   /page_vfzvzhyx  0.061475               NaN  7gtvvylw35cj              NaN       0.463011             NaN               NaN\n",
      "9   /page_e9u26xeo  0.027003               NaN  mr5ayoas34pa              NaN       0.148575             NaN               NaN\n",
      "10  /page_hxvqnxxz  0.120345               NaN  rmezjdylxkt6              NaN       1.000000             NaN               NaN\n",
      "11  /page_pz3yh635  0.059368               NaN  9c78h37maq2z              NaN       0.443790             NaN               NaN\n",
      "12  /page_dndqper7  0.073345           1.93704  4uyycdsgd7nv         0.516252       0.571282             NaN               NaN\n",
      "13  /page_tzj68wez  0.072569               NaN  1kvjvfomnjk2              NaN       0.564205             NaN               NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # For easy timestamp parsing\n",
    "import numpy as np\n",
    "\n",
    "# --- Assumes 'page_graph' (from the crawl) ---\n",
    "# --- and 'pagerank_scores' (from PageRank) ---\n",
    "# --- are both in memory. ---\n",
    "\n",
    "print(\"--- Analyzing Node ID Change Frequency ---\")\n",
    "\n",
    "# Store our results here\n",
    "page_data = []\n",
    "\n",
    "# 1. Check if the required variables exist\n",
    "if 'page_graph' not in locals() or 'pagerank_scores' not in locals():\n",
    "    print(\"Error: 'page_graph' or 'pagerank_scores' not in memory.\")\n",
    "    print(\"Please re-run your crawler and PageRank scripts first.\")\n",
    "else:\n",
    "    for page_path, data in page_graph.items():\n",
    "        history = data['history']\n",
    "        \n",
    "        avg_change_interval = np.nan # Default for pages with < 2 history items\n",
    "        \n",
    "        if len(history) >= 2:\n",
    "            # 2. Parse timestamps\n",
    "            # We add the 'current' node_id to the history to get the most recent interval\n",
    "            current_node_info = {'node_id': data['node_id'], 'timestamp': pd.Timestamp.now(tz='UTC')}\n",
    "            \n",
    "            # Combine history and current data\n",
    "            all_node_events = history + [current_node_info]\n",
    "\n",
    "            # Convert all timestamps to datetime objects\n",
    "            timestamps = [\n",
    "                pd.to_datetime(event['timestamp']) for event in all_node_events\n",
    "            ]\n",
    "            \n",
    "            # 3. Calculate time differences (deltas)\n",
    "            # We sort just in case, though history is likely sorted\n",
    "            timestamps.sort() \n",
    "            deltas_in_seconds = [\n",
    "                (timestamps[i] - timestamps[i-1]).total_seconds() \n",
    "                for i in range(1, len(timestamps))\n",
    "            ]\n",
    "            \n",
    "            # 4. Get the average\n",
    "            if deltas_in_seconds:\n",
    "                avg_change_interval = np.mean(deltas_in_seconds)\n",
    "        \n",
    "        # 5. Store the results\n",
    "        page_data.append({\n",
    "            'page_path': page_path,\n",
    "            'pagerank': pagerank_scores.get(page_path, 0),\n",
    "            'avg_interval_sec': avg_change_interval,\n",
    "            'last_node_id': data['node_id']\n",
    "        })\n",
    "\n",
    "    # --- Display the new data ---\n",
    "    \n",
    "    # Create a DataFrame for easy viewing\n",
    "    df_page_data = pd.DataFrame(page_data)\n",
    "    \n",
    "    # Calculate a simple \"freshness\" (1 / interval)\n",
    "    # Smaller interval = higher freshness\n",
    "    df_page_data['freshness_score'] = 1 / df_page_data['avg_interval_sec']\n",
    "    \n",
    "    # Normalize PageRank and Freshness to 0-1 range for a combined score\n",
    "    df_page_data['pagerank_norm'] = (\n",
    "        df_page_data['pagerank'] - df_page_data['pagerank'].min()\n",
    "    ) / (df_page_data['pagerank'].max() - df_page_data['pagerank'].min())\n",
    "    \n",
    "    df_page_data['freshness_norm'] = (\n",
    "        df_page_data['freshness_score'] - df_page_data['freshness_score'].min()\n",
    "    ) / (df_page_data['freshness_score'].max() - df_page_data['freshness_score'].min())\n",
    "\n",
    "    # --- This is the core of your revisit strategy ---\n",
    "    # We combine PageRank and Freshness\n",
    "    df_page_data['revisit_priority'] = (\n",
    "        df_page_data['pagerank_norm'] + df_page_data['freshness_norm']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Page Analysis Complete ---\")\n",
    "    print(df_page_data.sort_values(by='revisit_priority', ascending=False).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
